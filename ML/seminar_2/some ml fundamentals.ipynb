{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d070513",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Формулировка задачи\n",
    "\n",
    "1. Supervised vs Unsupervised vs Reinforcement Learning\n",
    "\n",
    "2. Unsupervised Learning: Clustering vs Dimentionality Reduction\n",
    "\n",
    "3. Clustering: Soft vs Hard\n",
    "\n",
    "4. Supervised Learning: Classification vs Regression\n",
    "\n",
    "5. Discriminative vs Generative Models\n",
    "\n",
    "6. Structured vs unstructured prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991569f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Рассмотрим только **задачу классификации и регрессии**.\n",
    "\n",
    "Пусть $\\{ (x_i, y_i) \\}_{i=1}^N $ это датасет независимых одинаково распределённых случайных объектов\n",
    "\n",
    "Или:  \n",
    "$X \\in R^{Nxd}$ - матрица признаков, где $d$ размерность признакового пространства и $N$ - количество объектов.  \n",
    "$Y \\in R^{N}$ - целевая переменная (вектор)\n",
    "\n",
    "Решая задачу классификации $Y \\in \\{0,1, ... C-1\\}^N$, где $C$ - это количество классов \n",
    "\n",
    "Мы хотим найти такой алгоритм $h \\in H$ который для каждого объекта находит правильный класс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532812c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Классификация**\n",
    "<img src=\"https://miro.medium.com/max/1200/1*4vnglpUYwoluB9jxI845gQ.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b6780",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Решая задачу регрессии $Y \\in R$,\n",
    "\n",
    "Мы хотим найти такой алгоритм $h \\in H$ который для каждого объекта минимизирует e.g. среднеквадратичную ошибку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc628ebf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Регрессия**\n",
    "<img src=\"https://miro.medium.com/max/1838/0*jCUlCrU9zAwvkqWt.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30001b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fefcce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Потери (Loss): R x R \\rightarrow R $ - функция потерь, которая оценивает насколько хорошим является наше предсказание для конкретных объектов.\n",
    "\n",
    "Некоторые функции потерь:\n",
    "* $Loss(\\hat y, y) = (\\hat y - y)^2$\n",
    "* $Loss(\\hat y, y) = |\\hat y - y|$\n",
    "* $Loss(\\hat y, y) = \\frac {|\\hat y - y|} {y}$\n",
    "* $Loss(\\hat y, y) = I[\\hat y \\neq y]$\n",
    "\n",
    "\n",
    "Для задачи бинарной классификации $y \\in {-1,1}$ переменная $z = yh(x)$ называется **отступом**. \n",
    "\n",
    "Положительный отступ означает успешную классификацию, негативный означает ошибку.\n",
    "\n",
    "$|yh(x)|$ это расстояние до границы принятия решения (decision boundary), которое может быть интерпретировано как уверенность модели в классификации объекта.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*4vnglpUYwoluB9jxI845gQ.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdea66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Функции потерь могут быть выражены в терминах отступа:\n",
    "\n",
    "* Hinge loss   $Loss(\\hat y, y) = max(0, 1 - yf(x))$ \n",
    "* AdaBoost loss  $Loss(\\hat y, y) = e^{-yh(x)}$ \n",
    "* Logistic loss  $Loss(\\hat y, y) = \\log(1 + e^{-yh(x)})$ \n",
    "* Classification error  $Loss(\\hat y, y) = I[yh(x) < 0]$  \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/0pREW.png\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7801c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Минимизация эмпирического риска\n",
    "\n",
    "\n",
    "$$\\hat R = \\sum_{i=1}^{N}Loss(x_i, y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116ec00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# No free lunch theorem\n",
    "\n",
    "<img src=\"https://miro.medium.com/proxy/1*0QP3OeK7BAOWGlUcDG6VSw.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e082e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Смещение и разброс\n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/8a/5a/dc/8a5adc940e417046663b15e25960c5cd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3de39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://neerc.ifmo.ru/wiki/images/f/fe/High_bias_learning_curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a7541",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Переобучение и генерализация\n",
    "\n",
    "Переобучение (overfitting) - ситуация, когда предсказания модели, обученной на трейн части, хуже на тестовой. \n",
    "Означает, что модель выучила данный датасет, и не достигла генерализации данных, которых она не видела, из того же распределения \n",
    "\n",
    "Все модели способны на переобучение\n",
    "<img src=\"https://i.ytimg.com/vi/BRdQcZCCSYA/maxresdefault.jpg\">\n",
    "\n",
    "В ответ существуют техники для регуляризации\n",
    "\n",
    "В основном, они соответствуют накладыванию ограничений на пространство гипотез, что позволяет упрощать генерализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28f140",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Валидация\n",
    "\n",
    "\n",
    "\n",
    "1. $\\{x_i, y_i\\}_{i=1}^N$ засемплированы из совместного распределения $P(x,y)$\n",
    "1. разделены на непересекающиеся сабсеты (трейн и тест)  \n",
    "1. модель h(x_i; w, \\theta) задана обучаемыми весами $w$ и необучаемыми гиперпараметрами $\\theta$  \n",
    "1. выбираем гиперпараметр $\\theta=\\theta_0$ и обучаем модель $$\\sum_{i \\in train} Loss( h(x_i; w, \\theta_0), y_i) \\rightarrow \\min_{w}$$\n",
    "1. тестируем на тестовом датасете $$\\hat R(\\theta_0) = \\sum_{i \\in test} Loss( h(x_i; w, \\theta_0), y_i)$$\n",
    "1. ожидаем что получили достаточную аппроксимацию ошибки генерализации $$R(\\theta_0) = E_{(x,y) \\sim P(x,y)} [ Loss( h(x_i; w, \\theta_0), y_i)]$$\n",
    "1. Как выбрать $\\theta$?\n",
    "\n",
    "Обычно мы стремимся оптимизировать гиперпараметры, попробовав несколько by значений для $\\theta$ на тестовом датасете и выбрав лучшее.\n",
    "\n",
    "Но качество на тестовых данных - недетерменированная переменная  $\\hat R(\\theta)$, которая может \"скакать\" в зависимости от конкретного разбиения\n",
    "\n",
    "Приходит на помощь валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bcf54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hold Out\n",
    "\n",
    "<img src=\"https://puikjes.net/files/python/train_test_split-method-of-scikit-learn.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "415f6ca6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# k-fold Кросс-валидация\n",
    "\n",
    "\n",
    "k = количество разбиений\n",
    "разбиение = непересекающиеся подмножества данных  \n",
    "\n",
    "Имея датасет, состоящий из m объектов, создадим k экспериментов:  \n",
    "1. создаем сплит k-1:1\n",
    "1. обучаемся на k-1 разбиений\n",
    "1. оцениваем по k-му разбиению\n",
    "1. меняем сплит\n",
    "\n",
    "Усредняем качество на всех сплитах\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1146/1*53jLClCYRiSygwLs6Rrv_g.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa0dbf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d80d067e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "(100,)\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_classes=2)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a3f3e74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 20)\n",
      "(33, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e299a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x7f2133866430>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "kf.split(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c85650b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a757b81",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575757575757576"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4cbc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Sklearn k-fold viz](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html \"Standford\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
